{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1irHPA2xZC1AZKSnpWzo3Z_nb4Id48bzY","authorship_tag":"ABX9TyPk74sbiixVQDr6UWqNXYK+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import math\n","import os\n","import gc\n","import time\n","import re\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from google.colab import drive\n",""],"metadata":{"id":"N3AQXdp0RhzY","executionInfo":{"status":"ok","timestamp":1688085675567,"user_tz":240,"elapsed":151,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["try:\n","    %tensorflow_version 2.x\n","except:\n","    pass\n","import tensorflow as tf\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras import backend as K\n","import tensorflow_datasets as tfds\n","\n","from keras.preprocessing.text import Tokenizer\n","# from keras.preprocessing.sequence import pad_sequences\n","from keras_preprocessing.sequence import pad_sequences"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":461},"id":"ROC0auC0RhxS","executionInfo":{"status":"error","timestamp":1688085685124,"user_tz":240,"elapsed":9558,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}},"outputId":"834ff320-b738-4fbc-e5ff-5a584eb832a4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"]},{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-6aa72656cffa>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mImportError\u001b[0m: cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (/usr/local/lib/python3.10/dist-packages/keras/preprocessing/sequence.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["# Parameters for our model\n","INPUT_COLUMN = 'input'\n","TARGET_COLUMN = 'target'\n","NUM_SAMPLES = 80000 #40000\n","MAX_VOCAB_SIZE = 2**14\n","\n","BATCH_SIZE = 64  # Batch size for training.\n","EPOCHS = 10  # Number of epochs to train for.\n","MAX_LENGTH = 15\n","\n","# # Global parameters\n","# root_folder='/content/drive'\n","# data_folder_name='My Drive/datasets/eng_spa_translations'\n","checkpoint_folder = \"/content/drive/MyDrive/AI_Research/Transformer/Checkpoint\"\n","train_filename='spa.txt'\n","\n","# # Variable for data directory\n","# DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n","# train_filenamepath = os.path.abspath(os.path.join(DATA_PATH, train_filename))\n","# checkpoint_path = os.path.abspath(os.path.join(root_folder, checkpoint_folder))\n","\n","# Both train and test set are in the root data directory\n","# train_path = DATA_PATH\n","\n","train_path = '/content/drive/MyDrive/AI_Research/Transformer/Data'\n","DATA_PATH = train_path\n","train_filenamepath = os.path.abspath(os.path.join(DATA_PATH, train_filename))\n"],"metadata":{"id":"2nWnEv9aRhvf","executionInfo":{"status":"ok","timestamp":1688085923305,"user_tz":240,"elapsed":115,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["def preprocess_text_nonbreaking(corpus, non_breaking_prefixes):\n","  corpus_cleaned = corpus\n","  # Add the string $$$ before the non breaking prefixes\n","  # To avoid remove dots from some words\n","  for prefix in non_breaking_prefixes:\n","    corpus_cleaned = corpus_cleaned.replace(prefix, prefix + '$$$')\n","  # Remove dots not at the end of a sentence\n","  corpus_cleaned = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_cleaned)\n","  # Remove the $$$ mark\n","  corpus_cleaned = re.sub(r\"\\.$$$\", '', corpus_cleaned)\n","  # Rmove multiple white spaces\n","  corpus_cleaned = re.sub(r\"  +\", \" \", corpus_cleaned)\n","\n","  return corpus_cleaned"],"metadata":{"id":"uPShaaiYRhtZ","executionInfo":{"status":"ok","timestamp":1688085688750,"user_tz":240,"elapsed":3,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["with open(DATA_PATH+\"/nonbreaking_prefix.en\",\n","          mode = \"r\", encoding = \"utf-8\") as f:\n","    non_breaking_prefix_en = f.read()\n","with open(DATA_PATH+\"/nonbreaking_prefix.es\",\n","          mode = \"r\", encoding = \"utf-8\") as f:\n","    non_breaking_prefix_es = f.read()\n","\n","non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n","non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n","non_breaking_prefix_es = non_breaking_prefix_es.split(\"\\n\")\n","non_breaking_prefix_es = [' ' + pref + '.' for pref in non_breaking_prefix_es]\n",""],"metadata":{"id":"e8IZgBCJRhrT","executionInfo":{"status":"ok","timestamp":1688085689933,"user_tz":240,"elapsed":1028,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\n","# Load the dataset: sentence in english, sentence in spanish\n","df=pd.read_csv(train_filenamepath, sep=\"\\t\", header=None, names=[INPUT_COLUMN,TARGET_COLUMN], usecols=[0,1],\n","               nrows=NUM_SAMPLES)\n","# Preprocess the input data\n","input_data=df[INPUT_COLUMN].apply(lambda x : preprocess_text_nonbreaking(x, non_breaking_prefix_en)).tolist()\n","# Preprocess and include the end of sentence token to the target text\n","target_data=df[TARGET_COLUMN].apply(lambda x : preprocess_text_nonbreaking(x, non_breaking_prefix_es)).tolist()\n","\n","print('Number of sentences: ',len(input_data))\n","print(input_data[:5])\n","print(target_data[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x0GwnTCIVsNo","executionInfo":{"status":"ok","timestamp":1688085698518,"user_tz":240,"elapsed":8586,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}},"outputId":"4fdc327c-117f-485d-e0a3-88af8406f7fa"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of sentences:  80000\n","['Go', 'Go', 'Go', 'Go', 'Hi']\n","['Ve', 'Vete', 'Vaya', 'VÃ¡yase', 'Hola']\n"]}]},{"cell_type":"code","source":["#Delete the dataframe and release the memory (if it is possible)\n","del df\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RVBecNMsVsLk","executionInfo":{"status":"ok","timestamp":1688085698641,"user_tz":240,"elapsed":5,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}},"outputId":"b2a229d5-699b-48ec-9a37-180910b4b1ad"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["## Tokenize the Data"],"metadata":{"id":"JAhkuy6bVsJd"}},{"cell_type":"code","source":["def subword_tokenize(corpus, vocab_size, max_length):\n","  # Create the vocabulary using Subword tokenization\n","  tokenizer_corpus = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    corpus, target_vocab_size=vocab_size)\n","  # Get the final vocab size, adding the eos and sos tokens\n","  num_words = tokenizer_corpus.vocab_size + 2\n","  # Set eos and sos token\n","  sos_token = [num_words-2]\n","  eos_token = [num_words-1]\n","  # Tokenize the corpus\n","  sentences = [sos_token + tokenizer_corpus.encode(sentence) + eos_token\n","          for sentence in corpus]\n","  # Identify the index of the sentences longer than max length\n","  idx_to_remove = [count for count, sent in enumerate(sentences)\n","                 if len(sent) > max_length]\n","  #Pad the sentences\n","  sentences = tf.keras.preprocessing.sequence.pad_sequences(sentences,\n","                                                       value=0,\n","                                                       padding='post',\n","                                                       maxlen=max_length)\n","\n","  return sentences, tokenizer_corpus, num_words, sos_token, eos_token, idx_to_remove\n"],"metadata":{"id":"nAtORrwQVsHp","executionInfo":{"status":"ok","timestamp":1688085698641,"user_tz":240,"elapsed":1,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Tokenize and pad the input sequences\n","encoder_inputs, tokenizer_inputs, num_words_inputs, sos_token_input, eos_token_input, del_idx_inputs= subword_tokenize(input_data,\n","                                                                                                        MAX_VOCAB_SIZE, MAX_LENGTH)\n","# Tokenize and pad the outputs sequences\n","decoder_outputs, tokenizer_outputs, num_words_output, sos_token_output, eos_token_output, del_idx_outputs = subword_tokenize(target_data,\n","                                                                                                        MAX_VOCAB_SIZE, MAX_LENGTH)\n",""],"metadata":{"id":"3dmIVNQhVsFS","executionInfo":{"status":"ok","timestamp":1688085743783,"user_tz":240,"elapsed":45025,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Check the tokenize function\n","print(encoder_inputs[:5], sos_token_input, eos_token_input)\n","print(decoder_outputs[:5], sos_token_output, eos_token_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uV0ekIHFuL5y","executionInfo":{"status":"ok","timestamp":1688085743784,"user_tz":240,"elapsed":22,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}},"outputId":"c7eca101-ea94-430a-e247-7fd5fb1de77e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[[13542  5283 13543     0     0     0     0     0     0     0     0     0\n","      0     0     0]\n"," [13542  5283 13543     0     0     0     0     0     0     0     0     0\n","      0     0     0]\n"," [13542  5283 13543     0     0     0     0     0     0     0     0     0\n","      0     0     0]\n"," [13542  5283 13543     0     0     0     0     0     0     0     0     0\n","      0     0     0]\n"," [13542  1510 13543     0     0     0     0     0     0     0     0     0\n","      0     0     0]] [13542] [13543]\n","[[15145  7918 15146     0     0     0     0     0     0     0     0     0\n","      0     0     0]\n"," [15145  7918   242 15146     0     0     0     0     0     0     0     0\n","      0     0     0]\n"," [15145 10202 15146     0     0     0     0     0     0     0     0     0\n","      0     0     0]\n"," [15145 14124 15146     0     0     0     0     0     0     0     0     0\n","      0     0     0]\n"," [15145  1666 15146     0     0     0     0     0     0     0     0     0\n","      0     0     0]] [15145] [15146]\n"]}]},{"cell_type":"code","source":["print('Size of Input Vocabulary: ', num_words_inputs)\n","print('Size of Output Vocabulary: ', num_words_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rqw5okQnuL39","executionInfo":{"status":"ok","timestamp":1688085743784,"user_tz":240,"elapsed":19,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}},"outputId":"2b17a253-2c0a-4f27-8a44-8ecdbca33212"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of Input Vocabulary:  13544\n","Size of Output Vocabulary:  15147\n"]}]},{"cell_type":"markdown","source":["## Create Batch data Generator"],"metadata":{"id":"aAwTnaFquL14"}},{"cell_type":"code","source":["# Define a dataset\n","dataset = tf.data.Dataset.from_tensor_slices(\n","    (encoder_inputs, decoder_outputs))\n","dataset = dataset.shuffle(len(input_data), reshuffle_each_iteration=True).batch(\n","    BATCH_SIZE, drop_remainder=True)\n","\n","dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"],"metadata":{"id":"x9XYYrduudJb","executionInfo":{"status":"ok","timestamp":1688085747186,"user_tz":240,"elapsed":2806,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# 1. Encoder"],"metadata":{"id":"dj8w8z1RCZ3_"}},{"cell_type":"markdown","source":["## 1.1 Scaled dot product Attention"],"metadata":{"id":"KO1hpvDPCeII"}},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"q8cSm7PaCoRW","executionInfo":{"status":"ok","timestamp":1688085747187,"user_tz":240,"elapsed":4,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":14,"metadata":{"id":"7rsbR8Z9CGvR","executionInfo":{"status":"ok","timestamp":1688085747187,"user_tz":240,"elapsed":3,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"outputs":[],"source":["def scaled_dot_product_attention(queries, keys, values, mask):\n","    # Calculate the dot product, QK_transpose\n","    product = tf.matmul(queries, keys, transpose_b=True)\n","    # Get the scale factor\n","    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n","    # Apply the scale factor to the dot product\n","    scaled_product = product / tf.math.sqrt(keys_dim)\n","    # Apply masking when it is requiered\n","    if mask is not None:\n","        scaled_product += (mask * -1e9)\n","    # dot product with Values\n","    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n","\n","    return attention"]},{"cell_type":"code","source":["import keras.layers as layers\n","class MultiHeadAttention(layers.Layer):\n","\n","    def __init__(self, n_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.n_heads = n_heads\n","\n","    def build(self, input_shape):\n","        self.d_model = input_shape[-1]\n","        assert self.d_model % self.n_heads == 0\n","        # Calculate the dimension of every head or projection\n","        self.d_head = self.d_model // self.n_heads\n","        # Set the weight matrices for Q, K and V\n","        self.query_lin = layers.Dense(units=self.d_model)\n","        self.key_lin = layers.Dense(units=self.d_model)\n","        self.value_lin = layers.Dense(units=self.d_model)\n","        # Set the weight matrix for the output of the multi-head attention W0\n","        self.final_lin = layers.Dense(units=self.d_model)\n","\n","    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n","        # Set the dimension of the projections\n","        shape = (batch_size,\n","                 -1,\n","                 self.n_heads,\n","                 self.d_head)\n","        # Split the input vectors\n","        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n","        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n","\n","    def call(self, queries, keys, values, mask):\n","        # Get the batch size\n","        batch_size = tf.shape(queries)[0]\n","        # Set the Query, Key and Value matrices\n","        queries = self.query_lin(queries)\n","        keys = self.key_lin(keys)\n","        values = self.value_lin(values)\n","        # Split Q, K y V between the heads or projections\n","        queries = self.split_proj(queries, batch_size)\n","        keys = self.split_proj(keys, batch_size)\n","        values = self.split_proj(values, batch_size)\n","        # Apply the scaled dot product\n","        attention = scaled_dot_product_attention(queries, keys, values, mask)\n","        # Get the attention scores\n","        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n","        # Concat the h heads or projections\n","        concat_attention = tf.reshape(attention,\n","                                      shape=(batch_size, -1, self.d_model))\n","        # Apply W0 to get the output of the multi-head attention\n","        outputs = self.final_lin(concat_attention)\n","\n","        return outputs\n"],"metadata":{"id":"rCZGLeoaCkOG","executionInfo":{"status":"ok","timestamp":1688085747344,"user_tz":240,"elapsed":160,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["class PositionalEncoding(layers.Layer):\n","\n","    def __init__(self):\n","        super(PositionalEncoding, self).__init__()\n","\n","    def get_angles(self, pos, i, d_model): # pos: (seq_length, 1) i: (1, d_model)\n","        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n","        return pos * angles # (seq_length, d_model)\n","\n","    def call(self, inputs):\n","        # input shape batch_size, seq_length, d_model\n","        seq_length = inputs.shape.as_list()[-2]\n","        d_model = inputs.shape.as_list()[-1]\n","        # Calculate the angles given the input\n","        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n","                                 np.arange(d_model)[np.newaxis, :],\n","                                 d_model)\n","        # Calculate the positional encodings\n","        angles[:, 0::2] = np.sin(angles[:, 0::2])\n","        angles[:, 1::2] = np.cos(angles[:, 1::2])\n","        # Expand the encodings with a new dimension\n","        pos_encoding = angles[np.newaxis, ...]\n","\n","        return inputs + tf.cast(pos_encoding, tf.float32)"],"metadata":{"id":"6Cl7wPd7PSbm","executionInfo":{"status":"ok","timestamp":1688085747344,"user_tz":240,"elapsed":4,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["class EncoderLayer(layers.Layer):\n","\n","    def __init__(self, FFN_units, n_heads, dropout_rate):\n","        super(EncoderLayer, self).__init__()\n","        # Hidden units of the feed forward component\n","        self.FFN_units = FFN_units\n","        # Set the number of projectios or heads\n","        self.n_heads = n_heads\n","        # Dropout rate\n","        self.dropout_rate = dropout_rate\n","\n","    def build(self, input_shape):\n","        self.d_model = input_shape[-1]\n","        # Build the multihead layer\n","        self.multi_head_attention = MultiHeadAttention(self.n_heads)\n","        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n","        # Layer Normalization\n","        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n","        # Fully connected feed forward layer\n","        self.ffn1_relu = layers.Dense(units=self.FFN_units, activation=\"relu\")\n","        self.ffn2 = layers.Dense(units=self.d_model)\n","        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n","        # Layer normalization\n","        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n","\n","    def call(self, inputs, mask, training):\n","        # Forward pass of the multi-head attention\n","        attention = self.multi_head_attention(inputs,\n","                                              inputs,\n","                                              inputs,\n","                                              mask)\n","        attention = self.dropout_1(attention, training=training)\n","        # Call to the residual connection and layer normalization\n","        attention = self.norm_1(attention + inputs)\n","        # Call to the FC layer\n","        outputs = self.ffn1_relu(attention)\n","        outputs = self.ffn2(outputs)\n","        outputs = self.dropout_2(outputs, training=training)\n","        # Call to residual connection and the layer normalization\n","        outputs = self.norm_2(outputs + attention)\n","\n","        return outputs"],"metadata":{"id":"Cvd0ucY9PkI7","executionInfo":{"status":"ok","timestamp":1688085747345,"user_tz":240,"elapsed":5,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["class Encoder(layers.Layer):\n","\n","    def __init__(self,\n","                 n_layers,\n","                 FFN_units,\n","                 n_heads,\n","                 dropout_rate,\n","                 vocab_size,\n","                 d_model,\n","                 name=\"encoder\"):\n","        super(Encoder, self).__init__(name=name)\n","        self.n_layers = n_layers\n","        self.d_model = d_model\n","        # The embedding layer\n","        self.embedding = layers.Embedding(vocab_size, d_model)\n","        # Positional encoding layer\n","        self.pos_encoding = PositionalEncoding()\n","        self.dropout = layers.Dropout(rate=dropout_rate)\n","        # Stack of n layers of multi-head attention and FC\n","        self.enc_layers = [EncoderLayer(FFN_units,\n","                                        n_heads,\n","                                        dropout_rate)\n","                           for _ in range(n_layers)]\n","\n","    def call(self, inputs, mask, training):\n","        # Get the embedding vectors\n","        outputs = self.embedding(inputs)\n","        # Scale the embeddings by sqrt of d_model\n","        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        # Positional encodding\n","        outputs = self.pos_encoding(outputs)\n","        outputs = self.dropout(outputs, training)\n","        # Call the stacked layers\n","        for i in range(self.n_layers):\n","            outputs = self.enc_layers[i](outputs, mask, training)\n","\n","        return outputs"],"metadata":{"id":"n1OMcN5dPoMC","executionInfo":{"status":"ok","timestamp":1688085747345,"user_tz":240,"elapsed":4,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["# 2. Decoder"],"metadata":{"id":"3ad6JO4rPw1M"}},{"cell_type":"code","source":["\n","class DecoderLayer(layers.Layer):\n","\n","    def __init__(self, FFN_units, n_heads, dropout_rate):\n","        super(DecoderLayer, self).__init__()\n","        self.FFN_units = FFN_units\n","        self.n_heads = n_heads\n","        self.dropout_rate = dropout_rate\n","\n","    def build(self, input_shape):\n","        self.d_model = input_shape[-1]\n","\n","        # Self multi head attention, causal attention\n","        self.multi_head_causal_attention = MultiHeadAttention(self.n_heads)\n","        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n","        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n","\n","        # Multi head attention, encoder-decoder attention\n","        self.multi_head_enc_dec_attention = MultiHeadAttention(self.n_heads)\n","        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n","        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n","\n","        # Feed foward\n","        self.ffn1_relu = layers.Dense(units=self.FFN_units,\n","                                    activation=\"relu\")\n","        self.ffn2 = layers.Dense(units=self.d_model)\n","        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n","        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n","\n","    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n","        # Call the masked causal attention\n","        attention = self.multi_head_causal_attention(inputs,\n","                                                inputs,\n","                                                inputs,\n","                                                mask_1)\n","        attention = self.dropout_1(attention, training)\n","        # Residual connection and layer normalization\n","        attention = self.norm_1(attention + inputs)\n","        # Call the encoder-decoder attention\n","        attention_2 = self.multi_head_enc_dec_attention(attention,\n","                                                  enc_outputs,\n","                                                  enc_outputs,\n","                                                  mask_2)\n","        attention_2 = self.dropout_2(attention_2, training)\n","        # Residual connection and layer normalization\n","        attention_2 = self.norm_2(attention_2 + attention)\n","        # Call the Feed forward\n","        outputs = self.ffn1_relu(attention_2)\n","        outputs = self.ffn2(outputs)\n","        outputs = self.dropout_3(outputs, training)\n","        # Residual connection and layer normalization\n","        outputs = self.norm_3(outputs + attention_2)\n","\n","        return outputs"],"metadata":{"id":"AEvsQeyCPt1e","executionInfo":{"status":"ok","timestamp":1688085747345,"user_tz":240,"elapsed":4,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["class Decoder(layers.Layer):\n","\n","    def __init__(self,\n","                 n_layers,\n","                 FFN_units,\n","                 n_heads,\n","                 dropout_rate,\n","                 vocab_size,\n","                 d_model,\n","                 name=\"decoder\"):\n","        super(Decoder, self).__init__(name=name)\n","        self.d_model = d_model\n","        self.n_layers = n_layers\n","        # Embedding layer\n","        self.embedding = layers.Embedding(vocab_size, d_model)\n","        # Positional encoding layer\n","        self.pos_encoding = PositionalEncoding()\n","        self.dropout = layers.Dropout(rate=dropout_rate)\n","        # Stacked layers of multi-head attention and feed forward\n","        self.dec_layers = [DecoderLayer(FFN_units,\n","                                        n_heads,\n","                                        dropout_rate)\n","                           for _ in range(n_layers)]\n","\n","    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n","        # Get the embedding vectors\n","        outputs = self.embedding(inputs)\n","        # Scale by sqrt of d_model\n","        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        # Positional encodding\n","        outputs = self.pos_encoding(outputs)\n","        outputs = self.dropout(outputs, training)\n","        # Call the stacked layers\n","        for i in range(self.n_layers):\n","            outputs = self.dec_layers[i](outputs,\n","                                         enc_outputs,\n","                                         mask_1,\n","                                         mask_2,\n","                                         training)\n","\n","        return outputs"],"metadata":{"id":"dhm62MOzP1kP","executionInfo":{"status":"ok","timestamp":1688085747345,"user_tz":240,"elapsed":4,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["# 3.Transformer"],"metadata":{"id":"GKHXRiLAP81J"}},{"cell_type":"code","source":["class Transformer(tf.keras.Model):\n","\n","    def __init__(self,\n","                 vocab_size_enc,\n","                 vocab_size_dec,\n","                 d_model,\n","                 n_layers,\n","                 FFN_units,\n","                 n_heads,\n","                 dropout_rate,\n","                 name=\"transformer\"):\n","        super(Transformer, self).__init__(name=name)\n","        # Build the encoder\n","        self.encoder = Encoder(n_layers,\n","                               FFN_units,\n","                               n_heads,\n","                               dropout_rate,\n","                               vocab_size_enc,\n","                               d_model)\n","        # Build the decoder\n","        self.decoder = Decoder(n_layers,\n","                               FFN_units,\n","                               n_heads,\n","                               dropout_rate,\n","                               vocab_size_dec,\n","                               d_model)\n","        # build the linear transformation and softmax function\n","        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n","\n","    def create_padding_mask(self, seq): #seq: (batch_size, seq_length)\n","        # Create the mask for padding\n","        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","        return mask[:, tf.newaxis, tf.newaxis, :]\n","\n","    def create_look_ahead_mask(self, seq):\n","        # Create the mask for the causal attention\n","        seq_len = tf.shape(seq)[1]\n","        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","        return look_ahead_mask\n","\n","    def call(self, enc_inputs, dec_inputs, training):\n","        # Create the padding mask for the encoder\n","        enc_mask = self.create_padding_mask(enc_inputs)\n","        # Create the mask for the causal attention\n","        dec_mask_1 = tf.maximum(\n","            self.create_padding_mask(dec_inputs),\n","            self.create_look_ahead_mask(dec_inputs)\n","        )\n","        # Create the mask for the encoder-decoder attention\n","        dec_mask_2 = self.create_padding_mask(enc_inputs)\n","        # Call the encoder\n","        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n","        # Call the decoder\n","        dec_outputs = self.decoder(dec_inputs,\n","                                   enc_outputs,\n","                                   dec_mask_1,\n","                                   dec_mask_2,\n","                                   training)\n","        # Call the Linear and Softmax functions\n","        outputs = self.last_linear(dec_outputs)\n","\n","        return outputs"],"metadata":{"id":"UzOcrxobP4kH","executionInfo":{"status":"ok","timestamp":1688085747346,"user_tz":240,"elapsed":5,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["# 4.Training the Transformer"],"metadata":{"id":"3I8k367vQBCr"}},{"cell_type":"code","source":["def loss_function(target, pred):\n","    mask = tf.math.logical_not(tf.math.equal(target, 0))\n","    loss_ = loss_object(target, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_mean(loss_)"],"metadata":{"id":"qegVsp_JP-c2","executionInfo":{"status":"ok","timestamp":1688085747474,"user_tz":240,"elapsed":132,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","\n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(CustomSchedule, self).__init__()\n","\n","        self.d_model = tf.cast(d_model, tf.float32)\n","        self.warmup_steps = warmup_steps\n","\n","    def __call__(self, step):\n","        step = tf.cast(step, tf.float32)\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps**-1.5)\n","\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"],"metadata":{"id":"N2-BUuyPQC_1","executionInfo":{"status":"ok","timestamp":1688086983771,"user_tz":240,"elapsed":128,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["def main_train(dataset, transformer, n_epochs, print_every=50):\n","  ''' Train the transformer model for n_epochs using the data generator dataset'''\n","  losses = []\n","  accuracies = []\n","  # In every epoch\n","  for epoch in range(n_epochs):\n","    print(\"Starting epoch {}\".format(epoch+1))\n","    start = time.time()\n","    # Reset the losss and accuracy calculations\n","    train_loss.reset_states()\n","    train_accuracy.reset_states()\n","    # Get a batch of inputs and targets\n","    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n","        # Set the decoder inputs\n","        dec_inputs = targets[:, :-1]\n","        # Set the target outputs, right shifted\n","        dec_outputs_real = targets[:, 1:]\n","        with tf.GradientTape() as tape:\n","            # Call the transformer and get the predicted output\n","            predictions = transformer(enc_inputs, dec_inputs, True)\n","            # Calculate the loss\n","            loss = loss_function(dec_outputs_real, predictions)\n","        # Update the weights and optimizer\n","        gradients = tape.gradient(loss, transformer.trainable_variables)\n","        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","        # Save and store the metrics\n","        train_loss(loss)\n","        train_accuracy(dec_outputs_real, predictions)\n","\n","        if batch % print_every == 0:\n","            losses.append(train_loss.result())\n","            accuracies.append(train_accuracy.result())\n","            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n","                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n","\n","    # Checkpoint the model on every epoch\n","    ckpt_save_path = ckpt_manager.save()\n","    print(\"Saving checkpoint for epoch {} in {}\".format(epoch+1,\n","                                                        ckpt_save_path))\n","    print(\"Time for 1 epoch: {} secs\\n\".format(time.time() - start))\n","\n","  return losses, accuracies"],"metadata":{"id":"6nCDYNNhQFEn","executionInfo":{"status":"ok","timestamp":1688087015526,"user_tz":240,"elapsed":129,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["# Set hyperparamters for the model\n","D_MODEL = 512 # 512\n","N_LAYERS = 4 # 6\n","FFN_UNITS = 512 # 2048\n","N_HEADS = 8 # 8\n","DROPOUT_RATE = 0.1 # 0.1"],"metadata":{"id":"V-EMVe5rusqq","executionInfo":{"status":"ok","timestamp":1688087015827,"user_tz":240,"elapsed":1,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["\n","# Clean the session\n","tf.keras.backend.clear_session()\n","# Create the Transformer model\n","transformer = Transformer(vocab_size_enc=num_words_inputs,\n","                          vocab_size_dec=num_words_output,\n","                          d_model=D_MODEL,\n","                          n_layers=N_LAYERS,\n","                          FFN_units=FFN_UNITS,\n","                          n_heads=N_HEADS,\n","                          dropout_rate=DROPOUT_RATE)\n","\n","# Define a categorical cross entropy loss\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n","                                                            reduction=\"none\")\n","# Define a metric to store the mean loss of every epoch\n","train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n","# Define a matric to save the accuracy in every epoch\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n","# Create the scheduler for learning rate decay\n","leaning_rate = CustomSchedule(D_MODEL)\n","print(type(leaning_rate))\n","# Create the Adam optimizer\n","optimizer = tf.keras.optimizers.Adam(\n","                                    #  0.001,\n","                                     leaning_rate,\n","                                    #  tf.cast(leaning_rate, tf.float32),\n","                                     beta_1=0.9,\n","                                     beta_2=0.98,\n","                                     epsilon=1e-9)\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EQJ44QkTQlls","executionInfo":{"status":"ok","timestamp":1688087016256,"user_tz":240,"elapsed":2,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}},"outputId":"1150d81e-fb08-4de7-8a08-bb93c32c8da8"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["<class '__main__.CustomSchedule'>\n"]}]},{"cell_type":"code","source":["#Create the Checkpoint\n","ckpt = tf.train.Checkpoint(transformer=transformer,\n","                           optimizer=optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_folder, max_to_keep=5)\n","\n","if ckpt_manager.latest_checkpoint:\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\n","    print(\"Last checkpoint restored.\")\n",""],"metadata":{"id":"B_Ce_ncqQrgU","executionInfo":{"status":"ok","timestamp":1688087016996,"user_tz":240,"elapsed":111,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","losses, accuracies = main_train(dataset, transformer, EPOCHS, 100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yJYh8y8IvSiB","executionInfo":{"status":"ok","timestamp":1688094250548,"user_tz":240,"elapsed":4262788,"user":{"displayName":"Thejaka Mahaulpatha","userId":"04179683276855370298"}},"outputId":"f30e94f2-47a5-411f-c091-5710332fe5a8"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting epoch 1\n","Epoch 1 Batch 0 Loss 3.9732 Accuracy 0.0000\n","Epoch 1 Batch 100 Loss 3.9226 Accuracy 0.0531\n","Epoch 1 Batch 200 Loss 3.6759 Accuracy 0.0622\n","Epoch 1 Batch 300 Loss 3.4390 Accuracy 0.0687\n","Epoch 1 Batch 400 Loss 3.2363 Accuracy 0.0784\n","Epoch 1 Batch 500 Loss 3.0768 Accuracy 0.0872\n","Epoch 1 Batch 600 Loss 2.9495 Accuracy 0.0950\n","Epoch 1 Batch 700 Loss 2.8390 Accuracy 0.1021\n","Epoch 1 Batch 800 Loss 2.7453 Accuracy 0.1085\n","Epoch 1 Batch 900 Loss 2.6624 Accuracy 0.1142\n","Epoch 1 Batch 1000 Loss 2.5906 Accuracy 0.1193\n","Epoch 1 Batch 1100 Loss 2.5255 Accuracy 0.1243\n","Epoch 1 Batch 1200 Loss 2.4661 Accuracy 0.1288\n","Saving checkpoint for epoch 1 in /content/drive/MyDrive/AI_Research/Transformer/Checkpoint/ckpt-1\n","Time for 1 epoch: 763.6569299697876 secs\n","\n","Starting epoch 2\n","Epoch 2 Batch 0 Loss 1.7465 Accuracy 0.1842\n","Epoch 2 Batch 100 Loss 1.6995 Accuracy 0.1867\n","Epoch 2 Batch 200 Loss 1.6789 Accuracy 0.1887\n","Epoch 2 Batch 300 Loss 1.6581 Accuracy 0.1914\n","Epoch 2 Batch 400 Loss 1.6379 Accuracy 0.1943\n","Epoch 2 Batch 500 Loss 1.6172 Accuracy 0.1972\n","Epoch 2 Batch 600 Loss 1.5940 Accuracy 0.2000\n","Epoch 2 Batch 700 Loss 1.5735 Accuracy 0.2024\n","Epoch 2 Batch 800 Loss 1.5565 Accuracy 0.2050\n","Epoch 2 Batch 900 Loss 1.5399 Accuracy 0.2069\n","Epoch 2 Batch 1000 Loss 1.5240 Accuracy 0.2091\n","Epoch 2 Batch 1100 Loss 1.5091 Accuracy 0.2111\n","Epoch 2 Batch 1200 Loss 1.4953 Accuracy 0.2128\n","Saving checkpoint for epoch 2 in /content/drive/MyDrive/AI_Research/Transformer/Checkpoint/ckpt-2\n","Time for 1 epoch: 725.9695501327515 secs\n","\n","Starting epoch 3\n","Epoch 3 Batch 0 Loss 1.2633 Accuracy 0.2467\n","Epoch 3 Batch 100 Loss 1.1989 Accuracy 0.2450\n","Epoch 3 Batch 200 Loss 1.1988 Accuracy 0.2456\n","Epoch 3 Batch 300 Loss 1.1951 Accuracy 0.2458\n","Epoch 3 Batch 400 Loss 1.1948 Accuracy 0.2467\n","Epoch 3 Batch 500 Loss 1.1904 Accuracy 0.2474\n","Epoch 3 Batch 600 Loss 1.1906 Accuracy 0.2476\n","Epoch 3 Batch 700 Loss 1.1869 Accuracy 0.2482\n","Epoch 3 Batch 800 Loss 1.1823 Accuracy 0.2489\n","Epoch 3 Batch 900 Loss 1.1814 Accuracy 0.2493\n","Epoch 3 Batch 1000 Loss 1.1787 Accuracy 0.2496\n","Epoch 3 Batch 1100 Loss 1.1781 Accuracy 0.2499\n","Epoch 3 Batch 1200 Loss 1.1763 Accuracy 0.2501\n","Saving checkpoint for epoch 3 in /content/drive/MyDrive/AI_Research/Transformer/Checkpoint/ckpt-3\n","Time for 1 epoch: 714.9839081764221 secs\n","\n","Starting epoch 4\n","Epoch 4 Batch 0 Loss 0.9629 Accuracy 0.2612\n","Epoch 4 Batch 100 Loss 1.0367 Accuracy 0.2584\n","Epoch 4 Batch 200 Loss 1.0545 Accuracy 0.2586\n","Epoch 4 Batch 300 Loss 1.0639 Accuracy 0.2577\n","Epoch 4 Batch 400 Loss 1.0657 Accuracy 0.2584\n","Epoch 4 Batch 500 Loss 1.0700 Accuracy 0.2582\n","Epoch 4 Batch 600 Loss 1.0727 Accuracy 0.2582\n","Epoch 4 Batch 700 Loss 1.0706 Accuracy 0.2586\n","Epoch 4 Batch 800 Loss 1.0701 Accuracy 0.2588\n","Epoch 4 Batch 900 Loss 1.0701 Accuracy 0.2593\n","Epoch 4 Batch 1000 Loss 1.0693 Accuracy 0.2592\n","Epoch 4 Batch 1100 Loss 1.0694 Accuracy 0.2597\n","Epoch 4 Batch 1200 Loss 1.0686 Accuracy 0.2602\n","Saving checkpoint for epoch 4 in /content/drive/MyDrive/AI_Research/Transformer/Checkpoint/ckpt-4\n","Time for 1 epoch: 747.8654832839966 secs\n","\n","Starting epoch 5\n","Epoch 5 Batch 0 Loss 0.8500 Accuracy 0.2712\n","Epoch 5 Batch 100 Loss 0.9204 Accuracy 0.2757\n","Epoch 5 Batch 200 Loss 0.9222 Accuracy 0.2760\n","Epoch 5 Batch 300 Loss 0.9303 Accuracy 0.2755\n","Epoch 5 Batch 400 Loss 0.9378 Accuracy 0.2747\n","Epoch 5 Batch 500 Loss 0.9431 Accuracy 0.2742\n","Epoch 5 Batch 600 Loss 0.9462 Accuracy 0.2744\n","Epoch 5 Batch 700 Loss 0.9483 Accuracy 0.2743\n","Epoch 5 Batch 800 Loss 0.9499 Accuracy 0.2745\n","Epoch 5 Batch 900 Loss 0.9519 Accuracy 0.2744\n","Epoch 5 Batch 1000 Loss 0.9525 Accuracy 0.2742\n","Epoch 5 Batch 1100 Loss 0.9525 Accuracy 0.2744\n","Epoch 5 Batch 1200 Loss 0.9536 Accuracy 0.2745\n","Saving checkpoint for epoch 5 in /content/drive/MyDrive/AI_Research/Transformer/Checkpoint/ckpt-5\n","Time for 1 epoch: 698.3568556308746 secs\n","\n","Starting epoch 6\n","Epoch 6 Batch 0 Loss 0.9685 Accuracy 0.2812\n","Epoch 6 Batch 100 Loss 0.8595 Accuracy 0.2845\n","Epoch 6 Batch 200 Loss 0.8569 Accuracy 0.2861\n","Epoch 6 Batch 300 Loss 0.8556 Accuracy 0.2852\n","Epoch 6 Batch 400 Loss 0.8548 Accuracy 0.2847\n","Epoch 6 Batch 500 Loss 0.8566 Accuracy 0.2849\n","Epoch 6 Batch 600 Loss 0.8624 Accuracy 0.2844\n","Epoch 6 Batch 700 Loss 0.8639 Accuracy 0.2844\n","Epoch 6 Batch 800 Loss 0.8678 Accuracy 0.2843\n","Epoch 6 Batch 900 Loss 0.8700 Accuracy 0.2845\n","Epoch 6 Batch 1000 Loss 0.8708 Accuracy 0.2845\n","Epoch 6 Batch 1100 Loss 0.8726 Accuracy 0.2845\n","Epoch 6 Batch 1200 Loss 0.8739 Accuracy 0.2845\n","Saving checkpoint for epoch 6 in /content/drive/MyDrive/AI_Research/Transformer/Checkpoint/ckpt-6\n","Time for 1 epoch: 708.20037317276 secs\n","\n","Starting epoch 7\n","Epoch 7 Batch 0 Loss 0.7000 Accuracy 0.2812\n","Epoch 7 Batch 100 Loss 0.7637 Accuracy 0.2962\n","Epoch 7 Batch 200 Loss 0.7677 Accuracy 0.2968\n","Epoch 7 Batch 300 Loss 0.7763 Accuracy 0.2953\n","Epoch 7 Batch 400 Loss 0.7841 Accuracy 0.2944\n","Epoch 7 Batch 500 Loss 0.7908 Accuracy 0.2939\n","Epoch 7 Batch 600 Loss 0.7959 Accuracy 0.2931\n","Epoch 7 Batch 700 Loss 0.8005 Accuracy 0.2929\n","Epoch 7 Batch 800 Loss 0.8023 Accuracy 0.2929\n","Epoch 7 Batch 900 Loss 0.8058 Accuracy 0.2928\n","Epoch 7 Batch 1000 Loss 0.8076 Accuracy 0.2928\n","Epoch 7 Batch 1100 Loss 0.8105 Accuracy 0.2927\n","Epoch 7 Batch 1200 Loss 0.8116 Accuracy 0.2928\n","Saving checkpoint for epoch 7 in /content/drive/MyDrive/AI_Research/Transformer/Checkpoint/ckpt-7\n","Time for 1 epoch: 744.5269973278046 secs\n","\n","Starting epoch 8\n","Epoch 8 Batch 0 Loss 0.7592 Accuracy 0.3103\n","Epoch 8 Batch 100 Loss 0.7014 Accuracy 0.3045\n","Epoch 8 Batch 200 Loss 0.7210 Accuracy 0.3033\n","Epoch 8 Batch 300 Loss 0.7235 Accuracy 0.3026\n","Epoch 8 Batch 400 Loss 0.7297 Accuracy 0.3019\n","Epoch 8 Batch 500 Loss 0.7347 Accuracy 0.3015\n","Epoch 8 Batch 600 Loss 0.7395 Accuracy 0.3011\n","Epoch 8 Batch 700 Loss 0.7443 Accuracy 0.3009\n","Epoch 8 Batch 800 Loss 0.7461 Accuracy 0.3008\n","Epoch 8 Batch 900 Loss 0.7500 Accuracy 0.3004\n","Epoch 8 Batch 1000 Loss 0.7548 Accuracy 0.3000\n","Epoch 8 Batch 1100 Loss 0.7587 Accuracy 0.2999\n","Epoch 8 Batch 1200 Loss 0.7609 Accuracy 0.2999\n","Saving checkpoint for epoch 8 in /content/drive/MyDrive/AI_Research/Transformer/Checkpoint/ckpt-8\n","Time for 1 epoch: 721.3869824409485 secs\n","\n","Starting epoch 9\n","Epoch 9 Batch 0 Loss 0.6536 Accuracy 0.3158\n","Epoch 9 Batch 100 Loss 0.6725 Accuracy 0.3117\n","Epoch 9 Batch 200 Loss 0.6835 Accuracy 0.3109\n","Epoch 9 Batch 300 Loss 0.6860 Accuracy 0.3101\n","Epoch 9 Batch 400 Loss 0.6913 Accuracy 0.3083\n","Epoch 9 Batch 500 Loss 0.6956 Accuracy 0.3079\n","Epoch 9 Batch 600 Loss 0.7000 Accuracy 0.3076\n","Epoch 9 Batch 700 Loss 0.7042 Accuracy 0.3069\n","Epoch 9 Batch 800 Loss 0.7065 Accuracy 0.3065\n","Epoch 9 Batch 900 Loss 0.7093 Accuracy 0.3063\n","Epoch 9 Batch 1000 Loss 0.7131 Accuracy 0.3059\n","Epoch 9 Batch 1100 Loss 0.7170 Accuracy 0.3055\n","Epoch 9 Batch 1200 Loss 0.7190 Accuracy 0.3055\n","Saving checkpoint for epoch 9 in /content/drive/MyDrive/AI_Research/Transformer/Checkpoint/ckpt-9\n","Time for 1 epoch: 704.7683591842651 secs\n","\n","Starting epoch 10\n","Epoch 10 Batch 0 Loss 0.5576 Accuracy 0.3203\n","Epoch 10 Batch 100 Loss 0.6210 Accuracy 0.3166\n","Epoch 10 Batch 200 Loss 0.6292 Accuracy 0.3153\n","Epoch 10 Batch 300 Loss 0.6352 Accuracy 0.3147\n","Epoch 10 Batch 400 Loss 0.6439 Accuracy 0.3134\n","Epoch 10 Batch 500 Loss 0.6524 Accuracy 0.3128\n","Epoch 10 Batch 600 Loss 0.6597 Accuracy 0.3123\n","Epoch 10 Batch 700 Loss 0.6645 Accuracy 0.3118\n","Epoch 10 Batch 800 Loss 0.6691 Accuracy 0.3115\n","Epoch 10 Batch 900 Loss 0.6740 Accuracy 0.3110\n","Epoch 10 Batch 1000 Loss 0.6781 Accuracy 0.3109\n","Epoch 10 Batch 1100 Loss 0.6829 Accuracy 0.3104\n","Epoch 10 Batch 1200 Loss 0.6871 Accuracy 0.3101\n","Saving checkpoint for epoch 10 in /content/drive/MyDrive/AI_Research/Transformer/Checkpoint/ckpt-10\n","Time for 1 epoch: 703.0815441608429 secs\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ssDusayLvWJt"},"execution_count":null,"outputs":[]}]}